// Error handling and logging utilities
const { DEBUG_MODE } = require('./config.js');

/**
 * Custom error class for bot-specific errors
 */
class BotError extends Error {
  constructor(message, details = {}) {
    super(message);
    this.name = 'BotError';
    this.details = details;
    Error.captureStackTrace(this, BotError);
  }
}

/**
 * Logs an error with optional context and returns the error object
 * @param {string} message - Error message
 * @param {Error|object} error - Original error or error details
 * @param {object} context - Additional context for debugging
 * @returns {BotError} - The wrapped error
 */
function logError(message, error = {}, context = {}) {
  // Create a standardized error object
  const errorObj = error instanceof Error 
    ? { 
        message: error.message, 
        name: error.name, 
        stack: DEBUG_MODE ? error.stack : undefined 
      }
    : error;

  // Combine all information
  const fullError = {
    message,
    error: errorObj,
    context,
    timestamp: new Date().toISOString()
  };

  // Log the error
  console.error('ERROR:', JSON.stringify(fullError, null, 2));

  // Return as a BotError for consistent handling
  return new BotError(message, { originalError: error, context });
}

/**
 * Handles an error by returning it in a format the LLM can understand
 * @param {Error} error - The error to format
 * @returns {object} - Error info for the LLM
 */
function formatErrorForLLM(error) {
  return {
    error: true,
    message: error.message || 'An unknown error occurred',
    type: error.name || 'Error',
    details: error.details || {}
  };
}

/**
 * Handles an error by routing it through the LLM for response generation
 * This maintains the LLM-driven architecture by letting the LLM decide how to respond to errors
 * 
 * ⚠️ IMPORTANT: This approach is critical for our architecture - we NEVER use hardcoded responses
 * directly to Slack. All user-facing messages must be generated by the LLM dynamically based on context.
 * The LLM must also maintain its defined assistant role and never assume alternate personas
 * (such as "dev mode" triggered by special keys/phrases).
 * 
 * @param {Error} error - The error that occurred
 * @param {Object} slackContext - Context about the Slack environment (channel, thread, etc)
 * @returns {Promise<void>} - Resolves when error is handled
 */
async function handleErrorWithLLM(error, slackContext) {
  try {
    // Log the error first
    logError('Routing error to LLM for handling', error, slackContext);
    
    // Get the necessary modules
    const { getContextBuilder } = require('./contextBuilder.js');
    const contextBuilder = getContextBuilder();
    
    // Format the error for the LLM
    const formattedError = formatErrorForLLM(error);
    
    // Create a threadId from the context
    const threadId = slackContext.threadTs || slackContext.timestamp || slackContext.channelId;
    
    if (!threadId) {
      console.error('No thread ID available for error handling');
      return;
    }
    
    // Store the error context in the context builder
    contextBuilder.setMetadata(threadId, 'context', {
      ...slackContext,
      hasError: true,
      errorMessage: error.message,
      errorType: error.name || 'Error'
    });
    
    // Add error as system message for context
    contextBuilder.addMessage({
      source: 'system',
      id: `error_${Date.now()}`,
      timestamp: new Date().toISOString(),
      threadTs: threadId,
      text: `Error: ${error.message}`,
      type: 'error',
      metadata: {
        error: formattedError
      }
    });
    
    // Process the thread with the LLM to generate a response
    const { processThread } = require('./orchestrator.js');
    await processThread(threadId);
    
  } catch (handlerError) {
    console.error('Failed to handle error with LLM:', handlerError);
    // At this point, we have no choice but to fail silently, we've already tried our best
  }
}

module.exports = {
  BotError,
  logError,
  formatErrorForLLM,
  handleErrorWithLLM
}; 